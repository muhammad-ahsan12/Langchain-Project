# -*- coding: utf-8 -*-
"""multiplepdfs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNAerV3Aa-ks6anVLtuaMmtWYM48z7pX
"""

!pip install langchain
!pip install langchain-community
!pip install langchain-core
!pip install langchain_google_genai
!pip install Pypdf
!pip install sentence_transformers==2.2.2

!pip install faiss-gpu

from langchain.document_loaders import PyPDFDirectoryLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import PyPDFLoader
from langchain.memory import ConversationBufferMemory
import os

os.environ['GOOGLE_API_KEY'] = ''

import zipfile
import os

# Path to the uploaded zip file (it will be in /content if uploaded directly to Colab)
zip_path = '/content/pdfs.zip'

# Destination directory
extract_path = '/content/extracted_files'

# Create the destination directory if it does not exist
if not os.path.exists(extract_path):
    os.makedirs(extract_path)

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f'Files extracted to {extract_path}')

llm = ChatGoogleGenerativeAI(model = "gemini-1.5-pro",temperature=0.2)
# embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

"""# Load directrios which contain three pdfs
1.Lstm
2.AI internship agrement
3.python problems
"""

documents = PyPDFDirectoryLoader(extract_path).load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)

vectorstore = FAISS.from_documents(texts,embeddings)
vectorstore = vectorstore.as_retriever()

prompt_template = """
Use the following pieces of context to answer the question,
if you don't know the answer, just say that i don't know, don't try to make up an answer.
{context}
Question: {question}
"""
PROMPT = ChatPromptTemplate.from_template(prompt_template)

"""# Load_qa_chain"""

from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm,
                      chain_type="stuff",
                      prompt=PROMPT)
question = """Explain the LSTM"""
docs = vectorstore.get_relevant_documents(question)
result = chain.run(input_documents=docs, question=question)
print(result)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

"""# RetrievalQA Chain"""

from langchain.chains import RetrievalQA
chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore,
    chain_type_kwargs={"prompt": PROMPT},
    memory=memory
)
question = """Explain the LSTM"""
result = chain({"query": question})
print(result["result"])

"""# ConversationalRetrievalChain"""

from langchain.chains import RetrievalQA, ConversationalRetrievalChain
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore,
    combine_docs_chain_kwargs={"prompt": PROMPT},
    memory=memory
)

chat_history = []
question = """who is muhammad Ahsan and also tell me about maktek agreament"""
result = chain({"question": question,"chat_history":chat_history})
print(result["answer"])

memory.load_memory_variables({})

"""# ConversationChain"""

from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

conversation = ConversationChain(
    llm=llm,
    verbose=True,
    memory=ConversationBufferMemory()
)
conversation.predict(input="yes tell me about maktek agreament")

"""# Extract Data from websites and store in txt file and then load"""

import requests
from langchain.document_loaders import TextLoader

url = 'https://raw.githubusercontent.com/langchain-ai/langchain/fd7ab539c8d1ae6ba3b978eeb0adf4977485fdbb/docs/docs/modules/state_of_the_union.txt'
resp = requests.get(url)
with open('state_of_the_union.txt', 'wb') as f:
    f.write(resp.content)

loader = TextLoader('state_of_the_union.txt')
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(texts,embeddings)
vectorstore = vectorstore.as_retriever()
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore,
    combine_docs_chain_kwargs={"prompt": PROMPT},
    memory=memory
)
chat_history = []
question = """What did the president say about Ketanji Brown Jackson"""
result = chain({"question": question,"chat_history":chat_history})
print(result["answer"])

